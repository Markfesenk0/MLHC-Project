{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMB76+6XT+8gAUQTiwoShG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Markfesenk0/MLHC-Project/blob/main/MLHC_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Student Details"
      ],
      "metadata": {
        "id": "MbEOt5VNrtKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|        |             Full Name |             Id |             Email |\n",
        "|---------|-------------------|----------------|------------------ |\n",
        "|Student 1|   Mark Fesenko|  321208605|  markfesenko@mail.tau.ac.il|\n",
        "|Student 2|   |  |  |"
      ],
      "metadata": {
        "id": "RPPlH47frwO2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvP8BmmgrgEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a147c3-63f0-40dd-bb82-2531ad9a7545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.42.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.42.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.1)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.56.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->shap) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.colab import data_table\n",
        "from google.colab import widgets\n",
        "from google.colab.data_table import DataTable\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import brier_score_loss\n",
        "!pip install shap\n",
        "!pip install shap xgboost imbalanced-learn\n",
        "import shap\n",
        "\n",
        "DataTable.max_columns = 200\n",
        "\n",
        "project = 'mimic-iii-i'\n",
        "client = bigquery.Client(project=project)\n",
        "data_table.enable_dataframe_formatter()\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_target_onset = 48       # minimal time of target since admission (hours)\n",
        "pred_gap = 6                # minimal gap between prediction and target (hours)\n",
        "pred_window = 42            # duration of prediction window, use only data collected in the first 42 hours for prediction\n",
        "min_los = 48                # only patients with at least >= 48 hours of hospitalization data\n",
        "mortality_window = 30              # mortality during or after hospitalization <= 30 days\n",
        "prolonged_stay = 7          # prolonged stay > 7 days\n",
        "hospital_readmission = 30   # hospital readmission in <= 30 days after discharge (not to be confused with ICU readmission within the same hospital admission)"
      ],
      "metadata": {
        "id": "_q5onP3X-3aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download additional files neede for the project\n",
        "\n",
        "!gdown 1QBKgf3B_bOthZmMPu2NSDplo7DlcQ3Lo\n",
        "!unzip -o project_addons.zip\n",
        "\n",
        "plots_directory = './figures'\n",
        "os.makedirs(plots_directory, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pISdgUOd_Olu",
        "outputId": "f0a347d3-e8cc-416b-8716-892b45be4eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QBKgf3B_bOthZmMPu2NSDplo7DlcQ3Lo\n",
            "To: /content/project_addons.zip\n",
            "\r  0% 0.00/89.0k [00:00<?, ?B/s]\r100% 89.0k/89.0k [00:00<00:00, 78.8MB/s]\n",
            "Archive:  project_addons.zip\n",
            "  inflating: test_example.csv        \n",
            "  inflating: vital_metadata.csv      \n",
            "  inflating: initial_cohort.csv      \n",
            "  inflating: labs_metadata.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - Patients\n",
        "hospquery = \\\n",
        "\"\"\"\n",
        "SELECT admissions.subject_id, admissions.hadm_id\n",
        ", admissions.admittime, admissions.dischtime\n",
        ", admissions.ethnicity, admissions.deathtime\n",
        ", admissions.admission_type, admissions.insurance\n",
        ", patients.gender, patients.dob, patients.dod\n",
        "FROM `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "INNER JOIN `physionet-data.mimiciii_clinical.patients` patients\n",
        "    ON admissions.subject_id = patients.subject_id\n",
        "WHERE admissions.has_chartevents_data = 1\n",
        "ORDER BY admissions.subject_id, admissions.hadm_id, admissions.admittime;\n",
        "\"\"\"\n",
        "\n",
        "hosps = client.query(hospquery).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "hosps = hosps.sort_values('admittime')"
      ],
      "metadata": {
        "id": "WDBLlGp00OOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting valueable information from patients admissions:\n",
        "\n",
        "\n",
        "\n",
        "1.   Patients' age\n",
        "2.   Length of hospital stay (used for our target labels later, removed before classification)\n",
        "3.   Patients' mortality (also used for target labels and then removed)\n",
        "4.   One hot encoding of patients' ethnicity\n",
        "5.   Binary gender column\n",
        "6.   One hot encoding of patients' insurance and admission type\n",
        "\n"
      ],
      "metadata": {
        "id": "NKuxgFayE7xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feature Extraction\n",
        "\n",
        "# Generate feature columns for los, age and mortality\n",
        "def age(admittime, dob):\n",
        "    if admittime < dob:\n",
        "      return 0\n",
        "    return admittime.year - dob.year - ((admittime.month, admittime.day) < (dob.month, dob.day))\n",
        "\n",
        "hosps['age'] = hosps.apply(lambda row: age(row['admittime'], row['dob']), axis=1)\n",
        "hosps['los_hosp_hr'] = (hosps.dischtime - hosps.admittime).astype('timedelta64[h]')\n",
        "hosps['mort'] = np.where(~np.isnat(hosps.dod),1,0)\n",
        "\n",
        "# Ethnicity - one hot encoding\n",
        "hosps.ethnicity = hosps.ethnicity.str.lower()\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^white')),'ethnicity'] = 'white'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^black')),'ethnicity'] = 'black'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^hisp')) | (hosps.ethnicity.str.contains('^latin')),'ethnicity'] = 'hispanic'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^asia')),'ethnicity'] = 'asian'\n",
        "hosps.loc[~(hosps.ethnicity.str.contains('|'.join(['white', 'black', 'hispanic', 'asian']))),'ethnicity'] = 'other'\n",
        "hosps = pd.concat([hosps, pd.get_dummies(hosps['ethnicity'], prefix='eth')], axis = 1)\n",
        "\n",
        "# Gender to binary\n",
        "hosps['gender'] = np.where(hosps['gender']==\"M\", 1, 0)\n",
        "\n",
        "# admission type and insurance - one hot encoding\n",
        "df_cat = hosps[['admission_type', 'insurance']].copy()\n",
        "df_num = hosps.drop(['admission_type', 'insurance'], axis = 1)\n",
        "df_cat = pd.get_dummies(df_cat, drop_first=True)\n",
        "hosps = pd.concat([df_num, df_cat], axis = 1)"
      ],
      "metadata": {
        "id": "IO3kD0mVErgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting our target labels for the task:\n",
        "\n",
        "\n",
        "1.   Mortality - during hospitalization or up to 30 days after discharge\n",
        "2.   Prolonged stay - length of stay > 7 days\n",
        "3.   Hospital readmission - in 30 days after discharge (not to be confused with ICU readmission\n",
        "within the same hospital admission)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sf7-MCb8ElOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Target Labels\n",
        "\n",
        "hosps['mortality_label'] = np.zeros(hosps.shape[0])\n",
        "hosps['prolonged_stay_label'] = np.zeros(hosps.shape[0])\n",
        "hosps['readmit_label'] = np.zeros(hosps.shape[0])\n",
        "hosps['readmit_label'] = hosps['readmit_label'].astype(int)\n",
        "\n",
        "# mortality label\n",
        "days_until_death = (hosps['dod'] - hosps['dischtime'].dt.floor('D')).dt.days\n",
        "hosps['mortality_label'] = days_until_death.apply(lambda x: 1 if x <= mortality_window else 0)\n",
        "\n",
        "# prolonged stay label\n",
        "hosps['prolonged_stay_label'] = hosps['los_hosp_hr'].apply(lambda x: 1 if x > prolonged_stay * 24 else 0)\n",
        "hosps = hosps.sort_values(by=['subject_id', 'admittime'], ascending=[True, True])\n",
        "hosps.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# readmit label\n",
        "for idx in np.arange(1, hosps.shape[0]):\n",
        "    if hosps.subject_id[idx] == hosps.subject_id[idx - 1] and hosps.hadm_id[idx] != hosps.hadm_id[idx - 1]:\n",
        "        # using first discharge and second admission time\n",
        "        first_dis = hosps.dischtime[idx-1]\n",
        "        second_adm = hosps.admittime[idx]\n",
        "        # fliter less than 0 hours to ignore duplicates\n",
        "        if pd.Timedelta(0, unit='h') < second_adm - first_dis <= pd.Timedelta(hospital_readmission, unit='d'):\n",
        "            hosps.at[idx - 1, 'readmit_label'] = 1"
      ],
      "metadata": {
        "id": "5fUvocK8_Cpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform some basic patients exclusion from our data:\n",
        "\n",
        "1.   Only patients first admissions is used in the data\n",
        "2.   Patients with length of stay lower than 48 hours are omitted\n",
        "3.   Patients who died in the first 48 hours are also omitted\n",
        "4.   Later, NEWBORN patients will also be omitted\n",
        "\n"
      ],
      "metadata": {
        "id": "aaR3bW5hF8eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Patient Exclusion Criteria\n",
        "\n",
        "# include only first admissions\n",
        "hosps = hosps.sort_values('admittime').groupby('subject_id').first().reset_index()\n",
        "\n",
        "# dismiss newborns\n",
        "hosps = hosps[hosps['admission_type_NEWBORN'] != 1]\n",
        "hosps = hosps.drop(['admission_type_NEWBORN'], axis=1)\n",
        "\n",
        "# only patients who admitted for at least 48 hours\n",
        "hosps = hosps.loc[hosps['los_hosp_hr'] >= min_los]\n",
        "\n",
        "# exclude patients who died in the first 48 hours\n",
        "hosps = hosps.loc[(hosps['mort'] == 0) | (hosps['deathtime'] - hosps['admittime'] >= pd.Timedelta(min_target_onset, unit='h'))]\n",
        "\n",
        "# show statistics on labels\n",
        "mortality_label_counts = hosps['mortality_label'].value_counts()\n",
        "prolonged_stay_label_counts = hosps['prolonged_stay_label'].value_counts()\n",
        "readmit_label_counts = hosps['readmit_label'].value_counts()\n",
        "\n",
        "print(mortality_label_counts)\n",
        "print(prolonged_stay_label_counts)\n",
        "print(readmit_label_counts)"
      ],
      "metadata": {
        "id": "EKQ7Z5FbExfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611bd7c6-b51e-4b80-b78f-bbd6a9b45a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    22281\n",
            "1     3406\n",
            "Name: mortality_label, dtype: int64\n",
            "1    12936\n",
            "0    12751\n",
            "Name: prolonged_stay_label, dtype: int64\n",
            "0    24633\n",
            "1     1054\n",
            "Name: readmit_label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get patients' Lab test and Vital signs for more useful features"
      ],
      "metadata": {
        "id": "k5b7dfVKG7rl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - Lab data\n",
        "\n",
        "labquery = \\\n",
        "\"\"\"--sql\n",
        "  SELECT labevents.subject_id ,labevents.hadm_id ,labevents.charttime\n",
        "  , labevents.itemid, labevents.valuenum\n",
        "  FROM `physionet-data.mimiciii_clinical.labevents` labevents\n",
        "    INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "    ON labevents.subject_id = admissions.subject_id\n",
        "    AND labevents.hadm_id = admissions.hadm_id\n",
        "    AND labevents.charttime >= (admissions.admittime)\n",
        "    AND labevents.charttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "    AND itemid in UNNEST(@itemids)\n",
        "\"\"\"\n",
        "\n",
        "lavbevent_meatdata = pd.read_csv('labs_metadata.csv')\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ArrayQueryParameter(\"itemids\", \"INTEGER\", lavbevent_meatdata['itemid'].tolist()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "labs = client.query(labquery, job_config=job_config).result().to_dataframe().rename(str.lower, axis='columns')"
      ],
      "metadata": {
        "id": "BpxwEQPgGQbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - Vitals data\n",
        "\n",
        "vitquery = \\\n",
        "\"\"\"--sql\n",
        "-- Vital signs include heart rate, blood pressure, respiration rate, and temperature\n",
        "\n",
        "  SELECT chartevents.subject_id ,chartevents.hadm_id ,chartevents.charttime\n",
        "  , chartevents.itemid, chartevents.valuenum\n",
        "  FROM `physionet-data.mimiciii_clinical.chartevents` chartevents\n",
        "  INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "  ON chartevents.subject_id = admissions.subject_id\n",
        "  AND chartevents.hadm_id = admissions.hadm_id\n",
        "  AND chartevents.charttime >= (admissions.admittime)\n",
        "  AND chartevents.charttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "  AND itemid in UNNEST(@itemids)\n",
        "  -- exclude rows marked as error\n",
        "  AND chartevents.error IS DISTINCT FROM 1\n",
        "\"\"\"\n",
        "\n",
        "vital_meatdata = pd.read_csv('vital_metadata.csv')\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ArrayQueryParameter(\"itemids\", \"INTEGER\", vital_meatdata['itemid'].tolist()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "vits = client.query(vitquery, job_config=job_config).result().to_dataframe().rename(str.lower, axis='columns')"
      ],
      "metadata": {
        "id": "UXOW9osYGciB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter Lab tests and Vital signs according to given valid data value ranges"
      ],
      "metadata": {
        "id": "JnzGvxwuHEXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Filter Invalid Measurement\n",
        "\n",
        "labs = labs[labs['hadm_id'].isin(hosps['hadm_id'])]\n",
        "labs = pd.merge(labs, lavbevent_meatdata, on='itemid')\n",
        "labs = labs[labs['valuenum'].between(labs['min'], labs['max'], inclusive='both')]\n",
        "\n",
        "vits = vits[vits['hadm_id'].isin(hosps['hadm_id'])]\n",
        "vits = pd.merge(vits, vital_meatdata, on='itemid')\n",
        "vits = vits[vits['valuenum'].between(vits['min'], vits['max'], inclusive='both')]\n",
        "\n",
        "# converty units from F to C\n",
        "vits.loc[(vits['feature name'] == 'TempF'),'valuenum'] = (vits[vits['feature name'] == 'TempF']['valuenum']-32)/1.8\n",
        "vits.loc[vits['feature name'] == 'TempF','feature name'] = 'TempC'"
      ],
      "metadata": {
        "id": "ozMfb3QyGjsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve SOFA and SAPSII scores for each patient.\n",
        "\n",
        "\n",
        "*   SOFA (Sequential Organ Failure Assessment) score is a tool used to assess the severity of organ dysfunction in critically ill patients. It evaluated the function of different organ systems, where each of them is assigned with a score based on the patient's clinical parameters. These scores are added up to provide the overall SOFA score.\n",
        "*   SAPS II (Simplified Acute Physiology Score I) score is a scoring system that evaluates the severity of illness and predicts mortality in critically ill patients within the first 24 hours of admission to ICU.Takes into account different physiological variables.\n",
        "\n",
        "These two scores were precalculated and derived from the MIMIC III dataset by its creators, and are located in the 'physionet-data.mimiciii_derived' DB.\n",
        "\n"
      ],
      "metadata": {
        "id": "ms72hCS7HQpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - SOFA and SAPSII features (Mimic Derived)\n",
        "\n",
        "sofa_query = \\\n",
        "\"\"\"--sql\n",
        "    SELECT sofa.subject_id, sofa.hadm_id, sofa.sofa, sofa.icustay_id\n",
        "    FROM `physionet-data.mimiciii_derived.sofa` sofa\n",
        "        INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "        ON sofa.subject_id = admissions.subject_id\n",
        "        AND sofa.hadm_id = admissions.hadm_id\n",
        "\"\"\"\n",
        "\n",
        "sapsii_query = \\\n",
        "\"\"\"--sql\n",
        "    SELECT sapsii.subject_id, sapsii.hadm_id, sapsii.sapsii, sapsii.icustay_id\n",
        "    FROM `physionet-data.mimiciii_derived.sapsii` sapsii\n",
        "    INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "        ON sapsii.subject_id = admissions.subject_id\n",
        "        AND sapsii.hadm_id = admissions.hadm_id\n",
        "\"\"\"\n",
        "\n",
        "sofa = client.query(sofa_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "sapsii = client.query(sapsii_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "# only keep first icustay data\n",
        "sofa.sort_values(by=['subject_id', 'hadm_id', 'icustay_id'], inplace=True)\n",
        "sofa.reset_index(drop=True, inplace=True)\n",
        "sofa = sofa.drop_duplicates(subset=['subject_id', 'hadm_id'], keep='first')\n",
        "sofa = sofa.drop('icustay_id', axis=1)\n",
        "\n",
        "sapsii.sort_values(by=['subject_id', 'hadm_id', 'icustay_id'], inplace=True)\n",
        "sapsii.reset_index(drop=True, inplace=True)\n",
        "sapsii = sapsii.drop_duplicates(subset=['subject_id', 'hadm_id'], keep='first')\n",
        "sapsii = sapsii.drop('icustay_id', axis=1)"
      ],
      "metadata": {
        "id": "XqNV_Xb7-pIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve medication information for patients during the first 42 hours of their admission. We created 6 different categories of medications that can point to a certain problem of a patient, and if a patient was given any of these medications in the first 42 hours of his admission, we mark it and use it as a feature for our classification task (using a boolean matrix of subject_id as rows and medication categories as columns)."
      ],
      "metadata": {
        "id": "6gGEJUBvJCNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - Medication Features\n",
        "# create a boolean matrix of patients who were given any of the medications below\n",
        "\n",
        "medication_categories = {\n",
        "    \"vasopressors\": ['Norepinephrine', 'Epinephrine', 'Vasopressin'],\n",
        "    \"sedatives\": ['Propofol', 'Midazolam', 'Fentanyl'],\n",
        "    \"antibiotics\": ['Amoxicillin', 'Vancomycin', 'Piperacillin'],\n",
        "    \"antiarrhythmics\": ['Bretylium', 'Amiodarone', 'Lidocaine'],\n",
        "    \"anticoagulants\": ['Heparin', 'Warfarin'],\n",
        "    \"inotropes\": ['Dopamine', 'Dobutamine', 'Milrinone']\n",
        "}\n",
        "\n",
        "columns = list(medication_categories.keys())\n",
        "medications_df = pd.DataFrame(columns=['subject_id'] + columns)\n",
        "medications_df['subject_id'] = hosps['subject_id'].unique()\n",
        "medications_df.fillna(0, inplace=True)\n",
        "\n",
        "medication_to_itemid = {}\n",
        "\n",
        "# Populate the medication_to_itemid dictionary\n",
        "for category, medications in medication_categories.items():\n",
        "    for medication in medications:\n",
        "        query = f\"\"\"\n",
        "            SELECT itemid\n",
        "            FROM `physionet-data.mimiciii_clinical.d_items`\n",
        "            WHERE LOWER(label) LIKE '%{medication.lower()}%'\n",
        "        \"\"\"\n",
        "        query_job = client.query(query)\n",
        "        rows = list(query_job)\n",
        "        if rows:\n",
        "            rows = [row.itemid for row in rows]\n",
        "            rows = list(filter(lambda item: item > 30000, rows))\n",
        "            medication_to_itemid[medication] = rows\n",
        "\n",
        "for category, medications in medication_categories.items():\n",
        "    for medication in medications:\n",
        "        itemid = medication_to_itemid.get(medication)\n",
        "        if itemid is None or len(itemid) == 0:\n",
        "            continue\n",
        "        query = f\"\"\"\n",
        "            SELECT DISTINCT subject_id, hadm_id, charttime FROM (\n",
        "                SELECT inputevents_cv.subject_id, inputevents_cv.hadm_id, inputevents_cv.charttime FROM `physionet-data.mimiciii_clinical.inputevents_cv` inputevents_cv\n",
        "                JOIN (\n",
        "                    SELECT subject_id, hadm_id, MIN(admittime) AS first_admittime\n",
        "                    FROM `physionet-data.mimiciii_clinical.admissions`\n",
        "                    GROUP BY subject_id, hadm_id\n",
        "                ) admissions_first\n",
        "                    ON inputevents_cv.subject_id = admissions_first.subject_id\n",
        "                    AND inputevents_cv.hadm_id = admissions_first.hadm_id\n",
        "                    AND inputevents_cv.charttime >= (first_admittime)\n",
        "                    AND inputevents_cv.charttime <= DATE_ADD(first_admittime, INTERVAL 42 HOUR)\n",
        "                WHERE itemid IN ({', '.join(map(str, itemid))})\n",
        "                UNION ALL\n",
        "                SELECT inputevents_mv.subject_id, inputevents_mv.hadm_id, inputevents_mv.starttime as charttime FROM `physionet-data.mimiciii_clinical.inputevents_mv` inputevents_mv\n",
        "                JOIN (\n",
        "                    SELECT subject_id, hadm_id, MIN(admittime) AS first_admittime\n",
        "                    FROM `physionet-data.mimiciii_clinical.admissions`\n",
        "                    GROUP BY subject_id, hadm_id\n",
        "                ) admissions_first\n",
        "                    ON inputevents_mv.subject_id = admissions_first.subject_id\n",
        "                    AND inputevents_mv.hadm_id = admissions_first.hadm_id\n",
        "                    AND inputevents_mv.starttime >= (first_admittime)\n",
        "                    AND inputevents_mv.starttime <= DATE_ADD(first_admittime, INTERVAL 42 HOUR)\n",
        "                WHERE itemid IN ({', '.join(map(str, itemid))})\n",
        "            )\n",
        "        \"\"\"\n",
        "        query_job = client.query(query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "        # filter only data relevant to each patients first admission & first 42 hours\n",
        "        filtered_df = pd.merge(query_job, hosps, on=['subject_id', 'hadm_id'], how='right')\n",
        "        filtered_df = filtered_df[\n",
        "            (filtered_df['charttime'] >= filtered_df['admittime']) &\n",
        "            (filtered_df['charttime'] <= filtered_df['dischtime'])\n",
        "        ]\n",
        "        subject_ids = filtered_df['subject_id'].unique()\n",
        "        mask = medications_df['subject_id'].isin(subject_ids)\n",
        "        medications_df.loc[mask, category.lower()] = 1\n",
        "\n",
        "medications_df = medications_df.sort_values(by='subject_id')\n",
        "medications_df.to_csv('medications_df.csv', index=False)"
      ],
      "metadata": {
        "id": "HchTxgupzebF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieve procedure events information for patients during the first 42 hours of their admission. Again, we look at different procedures performed on patients in the first 42 hours of their admission and mark it as a feature for later, using a boolean matrix with subject_id for rows and the different procedures as columns."
      ],
      "metadata": {
        "id": "WBz0ffiUJqBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query DB - Procedure Events\n",
        "\n",
        "procedureevents_mv_query = \\\n",
        "\"\"\"\n",
        "SELECT DISTINCT procedureevents_mv.subject_id, procedureevents_mv.hadm_id, procedureevents_mv.starttime, ordercategoryname\n",
        "FROM `physionet-data.mimiciii_clinical.procedureevents_mv` procedureevents_mv\n",
        "JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "    ON procedureevents_mv.subject_id = admissions.subject_id\n",
        "    AND procedureevents_mv.starttime >= (admissions.admittime)\n",
        "    AND procedureevents_mv.starttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "ORDER BY procedureevents_mv.subject_id;\n",
        "\"\"\"\n",
        "query_job = client.query(procedureevents_mv_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "# filter only data relevant to each patients first admission & first 42 hours\n",
        "filtered_df = pd.merge(query_job, hosps, on=['subject_id', 'hadm_id'], how='right')\n",
        "filtered_df = filtered_df[\n",
        "    (filtered_df['starttime'] >= filtered_df['admittime']) &\n",
        "    (filtered_df['starttime'] <= filtered_df['dischtime'])\n",
        "]\n",
        "filtered_df = filtered_df[['subject_id', 'ordercategoryname']]\n",
        "\n",
        "columns = list(filtered_df['ordercategoryname'].unique())\n",
        "columns = list(set(value.lower() for value in columns))\n",
        "procedure_events_df = pd.DataFrame(columns=['subject_id'] + columns)\n",
        "procedure_events_df['subject_id'] = hosps['subject_id'].unique()\n",
        "procedure_events_df.fillna(0, inplace=True)\n",
        "\n",
        "for category in columns:\n",
        "    subject_ids = filtered_df.loc[filtered_df['ordercategoryname'].str.lower() == category, 'subject_id'].tolist()\n",
        "    mask = procedure_events_df['subject_id'].isin(subject_ids)\n",
        "    procedure_events_df.loc[mask, category.lower()] = 1\n",
        "\n",
        "procedure_events_df = procedure_events_df.sort_values(by='subject_id')\n",
        "procedure_events_df.to_csv('procedure_events_df.csv', index=False)"
      ],
      "metadata": {
        "id": "paVTFjp1mPdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging all of our data together: admission data with target labels, procedure events, medications, vitals and lab tests, sapsii and sofa features."
      ],
      "metadata": {
        "id": "TaJKH6BcKWum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merged_df = hosps\n",
        "merged_df = hosps.merge(procedure_events_df, how='left', on=['subject_id'])\n",
        "merged_df = merged_df.merge(medications_df, how='left', on=['subject_id'])\n",
        "\n",
        "vits['category'] = 'vits'\n",
        "vits_and_labs_df = pd.concat([vits, labs])\n",
        "\n",
        "vits_and_labs_df['feature name'] = vits_and_labs_df['feature name'].str.lower()\n",
        "grouped = vits_and_labs_df.groupby(['hadm_id', 'feature name', pd.Grouper(key='charttime', freq='42H')])                    # group by 'hadm_id', 'feature_name', and daily intervals\n",
        "aggregated = grouped['valuenum'].agg(['mean', 'max', 'min'])                                                                # mean, max, and min for each group\n",
        "pivoted = aggregated.pivot_table(index=['hadm_id'], columns='feature name', values=['mean', 'max', 'min'])                  # reshape\n",
        "pivoted.columns = ['_'.join(col).rstrip('_') for col in pivoted.columns]                                                    # change names to min_feature, max_feature...\n",
        "pivoted.reset_index(inplace=True)\n",
        "merged_df = merged_df.merge(pivoted, how='left', on=['hadm_id'])\n",
        "\n",
        "# fix weight column\n",
        "weights = vits.loc[vits['feature name'] == 'Weight'].groupby(['subject_id'])['valuenum'].agg(['first'])\n",
        "weights.rename(columns={'first': 'weight'}, inplace=True)\n",
        "weights.reset_index(inplace=True)\n",
        "weights.rename(columns={'index': 'subject_id'}, inplace=True)\n",
        "merged_df = merged_df.drop(['min_weight', 'max_weight', 'mean_weight'], axis=1)\n",
        "merged_df = merged_df.merge(weights, how='left', on='subject_id')\n",
        "\n",
        "# add sofa and sapsii\n",
        "merged_df = merged_df.merge(sofa, how='left', on=['subject_id', 'hadm_id'])\n",
        "merged_df = merged_df.merge(sapsii, how='left', on=['subject_id', 'hadm_id'])"
      ],
      "metadata": {
        "id": "CXDj_ZQzV3Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping all columns that might cause data leakage, and splitting the data to train and test according to given subject ids."
      ],
      "metadata": {
        "id": "WuhJamAZd8LQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocess\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "target_labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "columns_to_drop = ['admittime', 'dischtime', 'ethnicity', 'dob', 'dod', 'deathtime', 'mort', 'hadm_id', 'los_hosp_hr']\n",
        "numerical_columns = ['age', 'weight', 'max_albumin', 'max_anion gap', 'max_bicarbonate', 'max_bilirubin', 'max_bun', 'max_chloride', 'max_creatinine',\n",
        "     'max_diasbp', 'max_glucose', 'max_heartrate', 'max_hematocrit', 'max_hemoglobin', 'max_inr', 'max_lactate',\n",
        "     'max_magnesium', 'max_meanbp', 'max_phosphate', 'max_platelet', 'max_potassium', 'max_pt', 'max_ptt',\n",
        "     'max_resprate', 'max_sodium', 'max_spo2', 'max_sysbp', 'max_tempc', 'max_wbc', 'mean_albumin', 'mean_anion gap',\n",
        "     'mean_bicarbonate', 'mean_bilirubin', 'mean_bun', 'mean_chloride', 'mean_creatinine', 'mean_diasbp',\n",
        "     'mean_glucose', 'mean_heartrate', 'mean_hematocrit', 'mean_hemoglobin', 'mean_inr', 'mean_lactate',\n",
        "     'mean_magnesium', 'mean_meanbp', 'mean_phosphate', 'mean_platelet', 'mean_potassium', 'mean_pt', 'mean_ptt',\n",
        "     'mean_resprate', 'mean_sodium', 'mean_spo2', 'mean_sysbp', 'mean_tempc', 'mean_wbc', 'min_albumin',\n",
        "     'min_anion gap', 'min_bicarbonate', 'min_bilirubin', 'min_bun', 'min_chloride', 'min_creatinine', 'min_diasbp',\n",
        "     'min_glucose', 'min_heartrate', 'min_hematocrit', 'min_hemoglobin', 'min_inr', 'min_lactate', 'min_magnesium',\n",
        "     'min_meanbp', 'min_phosphate', 'min_platelet', 'min_potassium', 'min_pt', 'min_ptt', 'min_resprate', 'min_sodium',\n",
        "     'min_spo2', 'min_sysbp', 'min_tempc', 'min_wbc']\n",
        "\n",
        "filtered_df = merged_df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "# filter initial cohort\n",
        "init_cohort = pd.read_csv('initial_cohort.csv')\n",
        "df_train = pd.merge(filtered_df, init_cohort, on='subject_id', how='inner')\n",
        "X_train = df_train.drop(['subject_id'] + target_labels, axis=1)\n",
        "y_train = df_train[target_labels]\n",
        "\n",
        "# filter test cohort\n",
        "test_cohort = pd.read_csv('test_example.csv')\n",
        "df_test = pd.merge(filtered_df, test_cohort, on='subject_id', how='inner')\n",
        "X_test = df_test.drop(['subject_id'] + target_labels, axis=1)\n",
        "y_test = df_test[target_labels]\n"
      ],
      "metadata": {
        "id": "BMuiLc-BQTmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impute data according to patients' age-gender"
      ],
      "metadata": {
        "id": "sBKOClhVeU6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imputation\n",
        "\n",
        "def impute(group):\n",
        "    \"\"\"\n",
        "    takes in a pandas group, and replaces the\n",
        "    null value with the mean of the none null\n",
        "    values of the same group\n",
        "    \"\"\"\n",
        "    mask = group.isnull()\n",
        "    group[mask] = group[~mask].mean()\n",
        "    return group\n",
        "\n",
        "filtered_columns = [column for column in numerical_columns if column != 'age']\n",
        "X_train['age_group'] = pd.cut(X_train['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69',  '70_79', '80p'], right = False)\n",
        "\n",
        "for item in filtered_columns:\n",
        "    try:\n",
        "        X_train[item.lower()] = X_train.groupby(['age_group', 'gender'])[item.lower()].transform(impute)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "X_train = X_train.drop('age_group', axis=1)\n",
        "\n",
        "# imputation for sofa, according to the creators of the SOFA score\n",
        "X_train['sofa'] = X_train['sofa'].fillna(0)\n",
        "X_train['sapsii'] = X_train['sapsii'].fillna(0)"
      ],
      "metadata": {
        "id": "X00aTmzoVHuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "class AgeGenderImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns): # If needed, you can add parameters to this function\n",
        "        self.imputers = None\n",
        "        self.columns = columns\n",
        "\n",
        "    # init imputers based on X (train)\n",
        "    def init_imputers(self):\n",
        "        X = X_train.copy()\n",
        "        self.imputers = {}\n",
        "        X['age_group'] = pd.cut(X_train['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69', '70_79', '80p'], right = False)\n",
        "        for (age, gender), group in X.groupby(['age_group', 'gender'])[self.columns]:\n",
        "            imputer = SimpleImputer(strategy='mean', keep_empty_features=True)\n",
        "            imputer.fit(group)\n",
        "            self.imputers[(age, gender)] = imputer\n",
        "\n",
        "    def fit(self, seen_data, y=None):\n",
        "        if self.imputers is None:\n",
        "            self.init_imputers()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_copy = X.copy()\n",
        "        X_copy['age_group'] = pd.cut(X_train['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69', '70_79', '80p'], right = False)\n",
        "        for (age, gender), group in X_copy.groupby(['age_group', 'gender'])[self.columns]:\n",
        "            if (age, gender) in self.imputers:\n",
        "                unique_rows = group[~group.index.duplicated(keep='first')]\n",
        "                imputed_group = self.imputers[(age, gender)].transform(group)\n",
        "                X_copy.loc[unique_rows.index, self.columns] = imputed_group\n",
        "        X_copy = X_copy.drop('age_group', axis=1)\n",
        "        return X_copy"
      ],
      "metadata": {
        "id": "TtIlK3ZnKaDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, roc_curve, auc\n",
        "from numpy import interp\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "def calibration_curve_plt(model, x_v, y_v):\n",
        "    # Predict probabilities using the calibrated pipeline\n",
        "    calibrated_probabilities = calibrated_pipeline.predict_proba(x_v)[:, 1]\n",
        "\n",
        "    # Calculate Brier score to evaluate calibration (lower is better)\n",
        "    brier_score = brier_score_loss(y_v, calibrated_probabilities)\n",
        "\n",
        "    y_pred_proba = pipeline.predict_proba(x_v)\n",
        "    prob_true, prob_pred = calibration_curve(y_v, y_pred_proba[:, 1], n_bins=10)\n",
        "\n",
        "    # Plot calibration curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\n",
        "    plt.plot([0, 1], [0, 1], ls='--', label='Perfectly Calibrated')\n",
        "    plt.xlabel('Mean Predicted Probability')\n",
        "    plt.ylabel('Fraction of Positives')\n",
        "    plt.title('Calibration Curve')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def roc_plot(model, x, y):\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fig = plt.figure(figsize=(7,7))\n",
        "    roc_aucs = []\n",
        "\n",
        "    pred_prob = model.predict_proba(x)\n",
        "    fpr, tpr, thresholds = roc_curve(y, pred_prob[:, 1])\n",
        "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    roc_aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=2, label='ROC (area = %0.2f)' % (roc_auc))\n",
        "\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Luck')\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.title('ROC curve: ' + label)\n",
        "\n",
        "    fig.savefig(plots_directory + '/ROC_' + label + '.png')\n",
        "\n",
        "def k_fold_roc(params, label, x_t, y_t, K = 5):\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fig = plt.figure(figsize=(7,7))\n",
        "\n",
        "    eval_size = int(np.round(1./K))\n",
        "    skf = StratifiedKFold(n_splits=K)\n",
        "    roc_aucs = []\n",
        "\n",
        "    for index, (train_indices, test_indices) in enumerate(skf.split(x_t, y_t[label])):\n",
        "        x, y = x_t.iloc[train_indices], y_t[label].iloc[train_indices]\n",
        "        X_valid, y_valid = x_t.iloc[test_indices], y_t[label].iloc[test_indices]\n",
        "\n",
        "        scale_weight = 1.*y.value_counts()[0]/y.value_counts()[1]\n",
        "        # params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "        model = XGBClassifier()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        imputer = AgeGenderImputer(numerical_columns)\n",
        "        scaler = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('numerical', StandardScaler(), numerical_columns)\n",
        "                ], remainder='passthrough')\n",
        "        scaler.fit(X_train)\n",
        "\n",
        "        pipeline = Pipeline(steps=[\n",
        "                    (\"imputer\", imputer),\n",
        "                    (\"scaler\", scaler),\n",
        "                    (\"smote\", SMOTE(sampling_strategy='not minority', random_state=42)),\n",
        "                    (\"model\", model)\n",
        "                ])\n",
        "\n",
        "        pipeline.fit(x, y)\n",
        "        pred_prob = pipeline.predict_proba(X_valid)\n",
        "        fpr, tpr, thresholds = roc_curve(y_valid, pred_prob[:, 1])\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "        mean_tpr[0] = 0.0\n",
        "\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_aucs.append(roc_auc)\n",
        "        plt.plot(fpr, tpr, lw=2, label='ROC fold %d (area = %0.2f)' % (index + 1, roc_auc))\n",
        "\n",
        "    mean_tpr /= K\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Luck')\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.title('ROC curve: ' + label)\n",
        "\n",
        "    fig.savefig(plots_directory + '/KFold_ROC_' + label + '.png')\n",
        "\n",
        "def feature_importance(model, X_train):\n",
        "    feature_importances = model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "    feature_importance_df = feature_importance_df.head(10)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title('Feature Importances - ' + label)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VMuEF2Oberb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = {'mortality_label': {'max_depth': 3,\n",
        "                                   'min_child_weight': 3,\n",
        "                                   'learning_rate': 0.1,\n",
        "                                   'n_estimators': 150,\n",
        "                                   'colsample_bytree': 0.75,\n",
        "                                   'gamma': 0.1,\n",
        "                                   'subsample': 0.75},\n",
        "               'prolonged_stay_label': {'max_depth': 3,\n",
        "                                        'min_child_weight': 5,\n",
        "                                        'learning_rate': 0.1,\n",
        "                                        'n_estimators': 150,\n",
        "                                        'colsample_bytree': 0.75,\n",
        "                                        'gamma': 0.5,\n",
        "                                        'subsample': 1},\n",
        "               'readmit_label': {'max_depth': 3,\n",
        "                                 'min_child_weight': 5,\n",
        "                                 'learning_rate': 0.1,\n",
        "                                 'n_estimators': 50,\n",
        "                                 'colsample_bytree': 0.75,\n",
        "                                 'gamma': 0.1,\n",
        "                                 'subsample': 1}}\n",
        "\n",
        "\n",
        "x_t, x_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "for label in y_train.columns.tolist()[::-1]:\n",
        "    params = best_params[label]\n",
        "\n",
        "    # scale weight of labels: number of negatives / nuber of positives\n",
        "    scale_weight = 1.*y_t[label].value_counts()[0]/y_t[label].value_counts()[1]\n",
        "    # params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "    model = XGBClassifier()\n",
        "    model.set_params(**params)\n",
        "    imputer = AgeGenderImputer(numerical_columns)\n",
        "    scaler = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('numerical', StandardScaler(), numerical_columns)\n",
        "            ], remainder='passthrough')\n",
        "    scaler.fit(x_t)\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "                (\"imputer\", imputer),\n",
        "                (\"scaler\", scaler),\n",
        "                (\"smote\", SMOTE(sampling_strategy='not minority', random_state=42)),\n",
        "                (\"model\", model)\n",
        "            ])\n",
        "\n",
        "    pipeline.fit(x_t, y_t[label])\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test[label], y_pred)\n",
        "    print(f\"Accuracy {label}: {accuracy:.2f}\", Counter(y_t[label]))\n",
        "\n",
        "    model = pipeline['model']\n",
        "    feature_importance(model, x_t)\n",
        "    calibration_curve_plt(pipeline, x_v, y_v[label])\n",
        "    k_fold_roc(params, label, x_t, y_t, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "wXcWCaD-aA_m",
        "outputId": "3bdd7dd1-4601-477c-f51c-b71b55f2f98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-258-ef57bde23742>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Calculate SHAP values for a single instance from the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Change this to the index of the desired sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "eQbmHdtJqRwR",
        "outputId": "a29e0a27-cbf2-4f4f-a774-2770a89471cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mortality_label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-250-17bf05fffdf3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# Change this to the index of the desired sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiabetes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mortality_label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a = pipeline['model'].estimator\n",
        "# a.feature_importances_\n",
        "\n",
        "fitted_classifier = pipeline.named_steps['model']\n",
        "feature_importances = fitted_classifier.feature_importances_"
      ],
      "metadata": {
        "id": "YJHhG08Rmvm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ],
      "metadata": {
        "id": "IK_ER14twrDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column_name in y_train.columns:\n",
        "    column_counts = y_train[column_name].value_counts()\n",
        "    print(f\"Counts for {column_name}:\\n{column_counts}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q28e2NoZJ2PA",
        "outputId": "84c499bc-983f-4980-b1cc-6c4420b28a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts for mortality_label:\n",
            "0    15642\n",
            "1     2418\n",
            "Name: mortality_label, dtype: int64\n",
            "\n",
            "Counts for prolonged_stay_label:\n",
            "1    9089\n",
            "0    8971\n",
            "Name: prolonged_stay_label, dtype: int64\n",
            "\n",
            "Counts for readmit_label:\n",
            "0    17319\n",
            "1      741\n",
            "Name: readmit_label, dtype: int64\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calibrated = CalibratedClassifierCV(model, method='sigmoid', cv=5)\n",
        "\n",
        "y_score_xgb_clb = cross_val_predict(estimator=calibrated, X=X, y=y, method='predict_proba', cv=5)\n",
        "\n",
        "plot_calibration_curves(y_true=y, y_prob=y_score_xgb_clb[:,1], n_bins=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "k4_gCw8lhVYP",
        "outputId": "16735573-ca28-4647-dc89-7a8e074f0b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-238-2ab73d0899e0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcalibrated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_score_xgb_clb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalibrated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict_proba'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_calibration_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_score_xgb_clb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cross_val_predict' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import brier_score_loss\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "for label in y_train.columns.tolist()[::-1]:\n",
        "    x_t, x_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    params = best_params[label]\n",
        "\n",
        "    # scale weight of labels: number of negatives / nuber of positives\n",
        "    scale_weight = 1.*y_train[label].value_counts()[0]/y_train[label].value_counts()[1]\n",
        "    # params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "    model = XGBClassifier()\n",
        "    model.set_params(**params)\n",
        "    imputer = AgeGenderImputer(numerical_columns)\n",
        "    scaler = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('numerical', StandardScaler(), numerical_columns)\n",
        "            ], remainder='passthrough')\n",
        "    scaler.fit(x_t)\n",
        "\n",
        "    pipeline = Pipeline(steps=[\n",
        "                (\"imputer\", imputer),\n",
        "                (\"scaler\", scaler),\n",
        "                # (\"smote\", SMOTE(random_state=42)),\n",
        "                (\"model\", model)\n",
        "            ])\n",
        "    pipeline.fit(x_t, y_t)\n",
        "\n",
        "    calibrated_pipeline = CalibratedClassifierCV(pipeline, method='sigmoid', cv=3)\n",
        "    calibrated_pipeline.fit(x_v, y_v[label])\n",
        "\n",
        "    # Predict probabilities using the calibrated pipeline\n",
        "    calibrated_probabilities = calibrated_pipeline.predict_proba(x_v)[:, 1]\n",
        "\n",
        "    # Calculate Brier score to evaluate calibration (lower is better)\n",
        "    brier_score = brier_score_loss(y_v[label], calibrated_probabilities)\n",
        "    calibration_curve_plt(calibrated_pipeline, x_v, y_v[label])\n",
        "    print(f\"Brier Score: {brier_score}\")\n",
        "    # roc(pipeline, X_test, y_test[label])"
      ],
      "metadata": {
        "id": "nZbBTzM6B9Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for column in y_train.columns:\n",
        "#     # Count the number of NaN values in the column\n",
        "#     num_nans = y_train[column].isna().sum()\n",
        "#     if num_nans > 0:\n",
        "#         # Print the column name and the number of NaN values\n",
        "#         print(f\"'{column}': {num_nans}\")"
      ],
      "metadata": {
        "id": "EWyWZKP3e91G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([X_train, y_train], axis = 1)\n",
        "\n",
        "# mortality in different ethnicities\n",
        "for eth in ['eth_white', 'eth_black', 'eth_hispanic', 'eth_asian', 'eth_other']:\n",
        "    text = eth + ': '\n",
        "    count = df[(df[eth] == 1) & (df['mortality_label'] == 1)].shape[0]\n",
        "    text += str(count) + ' died, out of ' + str(df[(df[eth] == 1)].shape[0]) + '. That\\'s '\n",
        "    percentage = round((count / df[(df[eth] == 1)].shape[0]) * 100, 2)\n",
        "    total_percentage = round(count / df[(df['mortality_label'] == 1)].shape[0] * 100, 2)\n",
        "    text += str(percentage) + '%, and ' + str(total_percentage) + '% of all dead'\n",
        "    print(text)\n",
        "\n",
        "# prolonged stay with newborns\n",
        "count = df[(df['admission_type_NEWBORN'] == 1) & (df['prolonged_stay_label'] == 1)].shape[0]\n",
        "percentage = round(count / df[(df['prolonged_stay_label'] == 1)].shape[0] * 100, 2)\n",
        "print(percentage, count, df[(df['prolonged_stay_label'] == 1)].shape[0])"
      ],
      "metadata": {
        "id": "ALIa5-KAw-VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "for label in labels:\n",
        "    best_params = {}\n",
        "    x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "    print(label)\n",
        "    model = XGBClassifier()\n",
        "    imputer = AgeGenderImputer(numerical_columns)\n",
        "    scaler = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('numerical', StandardScaler(), numerical_columns)\n",
        "            ], remainder='passthrough')\n",
        "    scaler.fit(x_t)\n",
        "    pipeline = Pipeline(steps=[\n",
        "            (\"imputer\", imputer),\n",
        "            (\"scaler\", scaler),\n",
        "            (\"model\", model)\n",
        "        ])\n",
        "\n",
        "    params = {\n",
        "        'model__max_depth': [3, 4, 5, 6],\n",
        "        'model__min_child_weight': [1, 3, 5]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=pipeline, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    best_params.update(cv_model.best_params_)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    params = {\n",
        "        'model__n_estimators': [50, 100, 150],\n",
        "        'model__learning_rate': [0.5, 0.1, 0.01]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    best_params.update(cv_model.best_params_)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    params = {\n",
        "        'model__gamma': [0.1, 0.5],\n",
        "        'model__subsample': [0.5, 0.75, 1],\n",
        "        'model__colsample_bytree': [0.75, 1]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    best_params.update(cv_model.best_params_)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    output_file = 'best_params_' + label + '.json'\n",
        "\n",
        "    # Write the merged_dict to a JSON file\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(best_params, json_file)"
      ],
      "metadata": {
        "id": "X1ukBzqrxm5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #    'objective':'binary:logistic',\n",
        "    #     'scale_pos_weight':scale_weight,\n",
        "    #     'min_child_weight': 5,\n",
        "    #     'seed' : 1,\n",
        "    #     'n_estimators' : 100,\n",
        "    #     'max_depth': 3,\n",
        "    #     'learning_rate':0.1,\n",
        "    #     'gamma':0.1,\n",
        "    #     'subsample': 0.6,\n",
        "    #     'colsample_bytree': 0.7,\n",
        "    #     'reg_alpha': 0.1\n",
        "\n",
        "labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "for label in labels:\n",
        "    x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "    print(label)\n",
        "    model = XGBClassifier()\n",
        "    imputer = AgeGenderImputer(numerical_columns)\n",
        "    scaler = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('numerical', StandardScaler(), numerical_columns)\n",
        "            ], remainder='passthrough')\n",
        "    scaler.fit(X_train)\n",
        "    pipeline = Pipeline(steps=[\n",
        "            (\"imputer\", imputer),\n",
        "            (\"scaler\", scaler),\n",
        "            (\"model\", model)\n",
        "        ])\n",
        "\n",
        "    params = {\n",
        "        'model__max_depth': [3, 4, 5, 6],\n",
        "        'model__min_child_weight': [1, 3, 5]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=pipeline, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    params = {\n",
        "        'model__n_estimators': [50, 100, 150],\n",
        "        'model__learning_rate': [0.5, 0.1, 0.01]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    params = {\n",
        "        'model__gamma': [0.1, 0.5],\n",
        "        'model__subsample': [0.5, 0.75, 1],\n",
        "        'model__colsample_bytree': [0.75, 1]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    print(label, cv_model.best_params_)\n",
        "\n",
        "\n",
        "# cv_model = GridSearchCV(estimator=pipeline, param_grid=params, cv=3, scoring='roc_auc')\n",
        "\n",
        "# cv_model.fit(X_train, y_train)\n",
        "\n",
        "# labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "# for label in labels:\n",
        "#     x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "#     print(label)\n",
        "#     model = XGBClassifier()\n",
        "#     imputer = AgeGenderImputer(numerical_columns)\n",
        "#     scaler = ColumnTransformer(\n",
        "#             transformers=[\n",
        "#                 ('numerical', StandardScaler(), numerical_columns)\n",
        "#             ], remainder='passthrough')\n",
        "#     scaler.fit(X_train)\n",
        "#     pipeline = Pipeline(steps=[\n",
        "#             (\"imputer\", imputer),\n",
        "#             (\"scaler\", scaler),\n",
        "#             (\"model\", model)\n",
        "#         ])\n",
        "\n",
        "#     params = {\n",
        "#         'model__max_depth': [3, 4],\n",
        "#         'model__min_child_weight': [1]\n",
        "#     }\n",
        "#     cv_model = GridSearchCV(estimator=pipeline, param_grid=params, cv=3, scoring='roc_auc')\n",
        "#     cv_model.fit(x_t, y_t)\n",
        "#     print(cv_model.best_params_)\n",
        "\n",
        "#     params = {\n",
        "#         'model__n_estimators': [50],\n",
        "#         'model__learning_rate': [0.5, 0.1]\n",
        "#     }\n",
        "#     cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "#     cv_model.fit(x_t, y_t)\n",
        "#     print(cv_model.best_params_)\n",
        "\n",
        "#     params = {\n",
        "#         'model__gamma': [0.1],\n",
        "#         'model__subsample': [0.5],\n",
        "#         'model__colsample_bytree': [0.75]\n",
        "#     }\n",
        "#     cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "#     cv_model.fit(x_t, y_t)\n",
        "#     print(cv_model.best_params_)\n"
      ],
      "metadata": {
        "id": "5I0mQ35iiXbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['admittime', 'dischtime', 'ethnicity', 'dob', 'dod', 'deathtime', 'mort', 'subject_id', 'hadm_id', 'los_hosp_hr']\n",
        "merged_df = merged_df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "columns_with_na = filter_df.columns[filter_df.isna().any()].tolist()\n",
        "columns_with_na = [string for string in columns_with_na if 'min_' not in string]\n",
        "columns_with_na = [string for string in columns_with_na if 'max_' not in string]\n",
        "columns_with_na = [string for string in columns_with_na if 'mean_' not in string]\n",
        "\n",
        "# Display the count of NaNs in each column\n",
        "print(columns_with_na)"
      ],
      "metadata": {
        "id": "uPDSgVOWvGS0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}