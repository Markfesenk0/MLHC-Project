{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Markfesenk0/MLHC-Project/blob/main/MLHC_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbEOt5VNrtKr"
      },
      "source": [
        "#Student Details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPPlH47frwO2"
      },
      "source": [
        "|        |             Full Name |             Id |             Email |\n",
        "|---------|-------------------|----------------|------------------ |\n",
        "|Student 1|   Mark Fesenko|  321208605|  markfesenko@mail.tau.ac.il|\n",
        "|Student 2|   |  |  |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "uvP8BmmgrgEg"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.colab import data_table\n",
        "from google.colab import widgets\n",
        "from google.colab.data_table import DataTable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "from datetime import timedelta\n",
        "from collections import Counter\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, roc_curve, auc, accuracy_score, brier_score_loss, precision_recall_curve, average_precision_score\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
        "from numpy import interp\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "!pip install shap\n",
        "!pip install shap xgboost imbalanced-learn\n",
        "import shap\n",
        "DataTable.max_columns = 200\n",
        "\n",
        "project = 'mimic-iii-i'\n",
        "client = bigquery.Client(project=project)\n",
        "data_table.enable_dataframe_formatter()\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_q5onP3X-3aH"
      },
      "outputs": [],
      "source": [
        "min_target_onset = 48       # minimal time of target since admission (hours)\n",
        "pred_gap = 6                # minimal gap between prediction and target (hours)\n",
        "pred_window = 42            # duration of prediction window, use only data collected in the first 42 hours for prediction\n",
        "min_los = 48                # only patients with at least >= 48 hours of hospitalization data\n",
        "mortality_window = 30              # mortality during or after hospitalization <= 30 days\n",
        "prolonged_stay = 7          # prolonged stay > 7 days\n",
        "hospital_readmission = 30   # hospital readmission in <= 30 days after discharge (not to be confused with ICU readmission within the same hospital admission)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pISdgUOd_Olu",
        "outputId": "6b83c2a6-8828-4c1a-f150-6f68b0f45e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QBKgf3B_bOthZmMPu2NSDplo7DlcQ3Lo\n",
            "To: /content/project_addons.zip\n",
            "\r  0% 0.00/89.0k [00:00<?, ?B/s]\r100% 89.0k/89.0k [00:00<00:00, 4.15MB/s]\n",
            "Archive:  project_addons.zip\n",
            "  inflating: test_example.csv        \n",
            "  inflating: vital_metadata.csv      \n",
            "  inflating: initial_cohort.csv      \n",
            "  inflating: labs_metadata.csv       \n"
          ]
        }
      ],
      "source": [
        "# download additional files neede for the project\n",
        "\n",
        "!gdown 1QBKgf3B_bOthZmMPu2NSDplo7DlcQ3Lo\n",
        "!unzip -o project_addons.zip\n",
        "\n",
        "plots_directory = './figures'\n",
        "os.makedirs(plots_directory, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WDBLlGp00OOU"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - Patients\n",
        "hospquery = \\\n",
        "\"\"\"\n",
        "SELECT admissions.subject_id, admissions.hadm_id\n",
        ", admissions.admittime, admissions.dischtime\n",
        ", admissions.ethnicity, admissions.deathtime\n",
        ", admissions.admission_type, admissions.insurance\n",
        ", patients.gender, patients.dob, patients.dod\n",
        "FROM `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "INNER JOIN `physionet-data.mimiciii_clinical.patients` patients\n",
        "    ON admissions.subject_id = patients.subject_id\n",
        "WHERE admissions.has_chartevents_data = 1\n",
        "ORDER BY admissions.subject_id, admissions.hadm_id, admissions.admittime;\n",
        "\"\"\"\n",
        "\n",
        "hosps = client.query(hospquery).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "hosps = hosps.sort_values('admittime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKuxgFayE7xT"
      },
      "source": [
        "Extracting valueable information from patients admissions:\n",
        "\n",
        "\n",
        "\n",
        "1.   Patients' age\n",
        "2.   Length of hospital stay (used for our target labels later, removed before classification)\n",
        "3.   Patients' mortality (also used for target labels and then removed)\n",
        "4.   One hot encoding of patients' ethnicity\n",
        "5.   Binary gender column\n",
        "6.   One hot encoding of patients' insurance and admission type\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IO3kD0mVErgj"
      },
      "outputs": [],
      "source": [
        "# @title Feature Extraction\n",
        "\n",
        "# Generate feature columns for los, age and mortality\n",
        "def age(admittime, dob):\n",
        "    if admittime < dob:\n",
        "      return 0\n",
        "    return admittime.year - dob.year - ((admittime.month, admittime.day) < (dob.month, dob.day))\n",
        "\n",
        "hosps['age'] = hosps.apply(lambda row: age(row['admittime'], row['dob']), axis=1)\n",
        "hosps['los_hosp_hr'] = (hosps.dischtime - hosps.admittime).astype('timedelta64[h]')\n",
        "hosps['mort'] = np.where(~np.isnat(hosps.dod),1,0)\n",
        "\n",
        "# Ethnicity - one hot encoding\n",
        "hosps.ethnicity = hosps.ethnicity.str.lower()\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^white')),'ethnicity'] = 'white'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^black')),'ethnicity'] = 'black'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^hisp')) | (hosps.ethnicity.str.contains('^latin')),'ethnicity'] = 'hispanic'\n",
        "hosps.loc[(hosps.ethnicity.str.contains('^asia')),'ethnicity'] = 'asian'\n",
        "hosps.loc[~(hosps.ethnicity.str.contains('|'.join(['white', 'black', 'hispanic', 'asian']))),'ethnicity'] = 'other'\n",
        "hosps = pd.concat([hosps, pd.get_dummies(hosps['ethnicity'], prefix='eth')], axis = 1)\n",
        "\n",
        "# Gender to binary\n",
        "hosps['gender'] = np.where(hosps['gender']==\"M\", 1, 0)\n",
        "\n",
        "# admission type and insurance - one hot encoding\n",
        "df_cat = hosps[['admission_type', 'insurance']].copy()\n",
        "df_num = hosps.drop(['admission_type', 'insurance'], axis = 1)\n",
        "df_cat = pd.get_dummies(df_cat, drop_first=True)\n",
        "hosps = pd.concat([df_num, df_cat], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7-MCb8ElOX"
      },
      "source": [
        "Extracting our target labels for the task:\n",
        "\n",
        "\n",
        "1.   Mortality - during hospitalization or up to 30 days after discharge\n",
        "2.   Prolonged stay - length of stay > 7 days\n",
        "3.   Hospital readmission - in 30 days after discharge (not to be confused with ICU readmission\n",
        "within the same hospital admission)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fUvocK8_Cpa",
        "outputId": "803fb9e8-5862-43e8-c5df-daa88fccbf8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    49335\n",
            "1     8049\n",
            "Name: mortality_label, dtype: int64\n",
            "0    30833\n",
            "1    26551\n",
            "Name: prolonged_stay_label, dtype: int64\n",
            "0    54319\n",
            "1     3065\n",
            "Name: readmit_label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# @title Target Labels\n",
        "\n",
        "labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "\n",
        "hosps['mortality_label'] = np.zeros(hosps.shape[0])\n",
        "hosps['prolonged_stay_label'] = np.zeros(hosps.shape[0])\n",
        "\n",
        "# mortality label\n",
        "days_until_death = (hosps['dod'] - hosps['dischtime'].dt.floor('D')).dt.days\n",
        "hosps['mortality_label'] = days_until_death.apply(lambda x: 1 if x <= mortality_window else 0)\n",
        "\n",
        "# prolonged stay label\n",
        "hosps['prolonged_stay_label'] = hosps['los_hosp_hr'].apply(lambda x: 1 if x > prolonged_stay * 24 else 0)\n",
        "\n",
        "# readmit label\n",
        "hosps = hosps.sort_values(by=['subject_id', 'admittime'])\n",
        "hosps['next_admittime'] = hosps.groupby('subject_id')['admittime'].shift(-1)\n",
        "hosps['time_to_next_admit'] = hosps['next_admittime'] - hosps['dischtime']\n",
        "hosps['readmit_label'] = (hosps['time_to_next_admit'] <= timedelta(days=hospital_readmission)).astype(int)\n",
        "hosps = hosps.drop(['next_admittime', 'time_to_next_admit'], axis=1)\n",
        "\n",
        "# show statistics on labels\n",
        "mortality_label_counts = hosps['mortality_label'].value_counts()\n",
        "prolonged_stay_label_counts = hosps['prolonged_stay_label'].value_counts()\n",
        "readmit_label_counts = hosps['readmit_label'].value_counts()\n",
        "print(mortality_label_counts)\n",
        "print(prolonged_stay_label_counts)\n",
        "print(readmit_label_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaR3bW5hF8eq"
      },
      "source": [
        "We perform some basic patients exclusion from our data:\n",
        "\n",
        "1.   Only patients first admissions is used in the data\n",
        "2.   Patients with length of stay lower than 48 hours are omitted\n",
        "3.   Patients who died in the first 48 hours are also omitted\n",
        "4.   Later, NEWBORN patients will also be omitted\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EKQ7Z5FbExfF"
      },
      "outputs": [],
      "source": [
        "# @title Patient Exclusion Criteria\n",
        "\n",
        "# age exclusion\n",
        "# hosps = hosps[hosps.age.between(17, 90, inclusive='neither')]\n",
        "\n",
        "# dismiss newborns?\n",
        "# hosps = hosps[hosps['admission_type_NEWBORN'] != 1]\n",
        "# hosps = hosps.drop(['admission_type_NEWBORN'], axis=1)\n",
        "\n",
        "# include only first admissions\n",
        "hosps = hosps.sort_values('admittime').groupby('subject_id').first().reset_index()\n",
        "\n",
        "# only patients who admitted for at least 48 hours\n",
        "hosps = hosps.loc[hosps['los_hosp_hr'] >= min_los]\n",
        "\n",
        "# exclude patients who died in the first 48 hours\n",
        "hosps = hosps.loc[(hosps['mort'] == 0) | (hosps['deathtime'] - hosps['admittime'] >= pd.Timedelta(min_target_onset, unit='h'))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5b7dfVKG7rl"
      },
      "source": [
        "Get patients' Lab test and Vital signs for more useful features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BpxwEQPgGQbR"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - Lab data\n",
        "\n",
        "labquery = \\\n",
        "\"\"\"--sql\n",
        "  SELECT labevents.subject_id ,labevents.hadm_id ,labevents.charttime\n",
        "  , labevents.itemid, labevents.valuenum\n",
        "  FROM `physionet-data.mimiciii_clinical.labevents` labevents\n",
        "    INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "    ON labevents.subject_id = admissions.subject_id\n",
        "    AND labevents.hadm_id = admissions.hadm_id\n",
        "    AND labevents.charttime >= (admissions.admittime)\n",
        "    AND labevents.charttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "    AND itemid in UNNEST(@itemids)\n",
        "\"\"\"\n",
        "\n",
        "lavbevent_meatdata = pd.read_csv('labs_metadata.csv')\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ArrayQueryParameter(\"itemids\", \"INTEGER\", lavbevent_meatdata['itemid'].tolist()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "labs = client.query(labquery, job_config=job_config).result().to_dataframe().rename(str.lower, axis='columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UXOW9osYGciB"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - Vitals data\n",
        "\n",
        "vitquery = \\\n",
        "\"\"\"--sql\n",
        "-- Vital signs include heart rate, blood pressure, respiration rate, and temperature\n",
        "\n",
        "  SELECT chartevents.subject_id ,chartevents.hadm_id ,chartevents.charttime\n",
        "  , chartevents.itemid, chartevents.valuenum\n",
        "  FROM `physionet-data.mimiciii_clinical.chartevents` chartevents\n",
        "  INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "  ON chartevents.subject_id = admissions.subject_id\n",
        "  AND chartevents.hadm_id = admissions.hadm_id\n",
        "  AND chartevents.charttime >= (admissions.admittime)\n",
        "  AND chartevents.charttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "  AND itemid in UNNEST(@itemids)\n",
        "  -- exclude rows marked as error\n",
        "  AND chartevents.error IS DISTINCT FROM 1\n",
        "\"\"\"\n",
        "\n",
        "vital_meatdata = pd.read_csv('vital_metadata.csv')\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    query_parameters=[\n",
        "        bigquery.ArrayQueryParameter(\"itemids\", \"INTEGER\", vital_meatdata['itemid'].tolist()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "vits = client.query(vitquery, job_config=job_config).result().to_dataframe().rename(str.lower, axis='columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnzGvxwuHEXl"
      },
      "source": [
        "Filter Lab tests and Vital signs according to given valid data value ranges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ozMfb3QyGjsn"
      },
      "outputs": [],
      "source": [
        "# @title Filter Invalid Measurement\n",
        "\n",
        "labs = labs[labs['hadm_id'].isin(hosps['hadm_id'])]\n",
        "labs = pd.merge(labs, lavbevent_meatdata, on='itemid')\n",
        "labs = labs[labs['valuenum'].between(labs['min'], labs['max'], inclusive='both')]\n",
        "\n",
        "vits = vits[vits['hadm_id'].isin(hosps['hadm_id'])]\n",
        "vits = pd.merge(vits, vital_meatdata, on='itemid')\n",
        "vits = vits[vits['valuenum'].between(vits['min'], vits['max'], inclusive='both')]\n",
        "\n",
        "# converty units from F to C\n",
        "vits.loc[(vits['feature name'] == 'TempF'),'valuenum'] = (vits[vits['feature name'] == 'TempF']['valuenum']-32)/1.8\n",
        "vits.loc[vits['feature name'] == 'TempF','feature name'] = 'TempC'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms72hCS7HQpD"
      },
      "source": [
        "Retrieve SOFA and SAPSII scores for each patient.\n",
        "\n",
        "\n",
        "*   SOFA (Sequential Organ Failure Assessment) score is a tool used to assess the severity of organ dysfunction in critically ill patients. It evaluated the function of different organ systems, where each of them is assigned with a score based on the patient's clinical parameters. These scores are added up to provide the overall SOFA score.\n",
        "*   SAPS II (Simplified Acute Physiology Score I) score is a scoring system that evaluates the severity of illness and predicts mortality in critically ill patients within the first 24 hours of admission to ICU.Takes into account different physiological variables.\n",
        "\n",
        "These two scores were precalculated and derived from the MIMIC III dataset by its creators, and are located in the 'physionet-data.mimiciii_derived' DB.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XqNV_Xb7-pIl"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - SOFA and SAPSII features (Mimic Derived)\n",
        "\n",
        "sofa_query = \\\n",
        "\"\"\"--sql\n",
        "    SELECT sofa.subject_id, sofa.hadm_id, sofa.sofa, sofa.icustay_id\n",
        "    FROM `physionet-data.mimiciii_derived.sofa` sofa\n",
        "        INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "        ON sofa.subject_id = admissions.subject_id\n",
        "        AND sofa.hadm_id = admissions.hadm_id\n",
        "\"\"\"\n",
        "\n",
        "sapsii_query = \\\n",
        "\"\"\"--sql\n",
        "    SELECT sapsii.subject_id, sapsii.hadm_id, sapsii.sapsii, sapsii.icustay_id\n",
        "    FROM `physionet-data.mimiciii_derived.sapsii` sapsii\n",
        "    INNER JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "        ON sapsii.subject_id = admissions.subject_id\n",
        "        AND sapsii.hadm_id = admissions.hadm_id\n",
        "\"\"\"\n",
        "\n",
        "sofa = client.query(sofa_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "sapsii = client.query(sapsii_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "# only keep first icustay data\n",
        "sofa.sort_values(by=['subject_id', 'hadm_id', 'icustay_id'], inplace=True)\n",
        "sofa.reset_index(drop=True, inplace=True)\n",
        "sofa = sofa.drop_duplicates(subset=['subject_id', 'hadm_id'], keep='first')\n",
        "sofa = sofa.drop('icustay_id', axis=1)\n",
        "\n",
        "sapsii.sort_values(by=['subject_id', 'hadm_id', 'icustay_id'], inplace=True)\n",
        "sapsii.reset_index(drop=True, inplace=True)\n",
        "sapsii = sapsii.drop_duplicates(subset=['subject_id', 'hadm_id'], keep='first')\n",
        "sapsii = sapsii.drop('icustay_id', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gGEJUBvJCNt"
      },
      "source": [
        "Retrieve medication information for patients during the first 42 hours of their admission. We created 6 different categories of medications that can point to a certain problem of a patient, and if a patient was given any of these medications in the first 42 hours of his admission, we mark it and use it as a feature for our classification task (using a boolean matrix of subject_id as rows and medication categories as columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HchTxgupzebF"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - Medication Features\n",
        "# create a boolean matrix of patients who were given any of the medications below\n",
        "\n",
        "medication_categories = {\n",
        "    \"vasopressors\": ['Norepinephrine', 'Epinephrine', 'Vasopressin'],\n",
        "    \"sedatives\": ['Propofol', 'Midazolam', 'Fentanyl'],\n",
        "    \"antibiotics\": ['Amoxicillin', 'Vancomycin', 'Piperacillin'],\n",
        "    \"antiarrhythmics\": ['Bretylium', 'Amiodarone', 'Lidocaine'],\n",
        "    \"anticoagulants\": ['Heparin', 'Warfarin'],\n",
        "    \"inotropes\": ['Dopamine', 'Dobutamine', 'Milrinone']\n",
        "}\n",
        "\n",
        "columns = list(medication_categories.keys())\n",
        "medications_df = pd.DataFrame(columns=['subject_id'] + columns)\n",
        "medications_df['subject_id'] = hosps['subject_id'].unique()\n",
        "medications_df.fillna(0, inplace=True)\n",
        "\n",
        "medication_to_itemid = {}\n",
        "\n",
        "# Populate the medication_to_itemid dictionary\n",
        "for category, medications in medication_categories.items():\n",
        "    for medication in medications:\n",
        "        query = f\"\"\"\n",
        "            SELECT itemid\n",
        "            FROM `physionet-data.mimiciii_clinical.d_items`\n",
        "            WHERE LOWER(label) LIKE '%{medication.lower()}%'\n",
        "        \"\"\"\n",
        "        query_job = client.query(query)\n",
        "        rows = list(query_job)\n",
        "        if rows:\n",
        "            rows = [row.itemid for row in rows]\n",
        "            rows = list(filter(lambda item: item > 30000, rows))\n",
        "            medication_to_itemid[medication] = rows\n",
        "\n",
        "for category, medications in medication_categories.items():\n",
        "    for medication in medications:\n",
        "        itemid = medication_to_itemid.get(medication)\n",
        "        if itemid is None or len(itemid) == 0:\n",
        "            continue\n",
        "        query = f\"\"\"\n",
        "            SELECT DISTINCT subject_id, hadm_id, charttime FROM (\n",
        "                SELECT inputevents_cv.subject_id, inputevents_cv.hadm_id, inputevents_cv.charttime FROM `physionet-data.mimiciii_clinical.inputevents_cv` inputevents_cv\n",
        "                JOIN (\n",
        "                    SELECT subject_id, hadm_id, MIN(admittime) AS first_admittime\n",
        "                    FROM `physionet-data.mimiciii_clinical.admissions`\n",
        "                    GROUP BY subject_id, hadm_id\n",
        "                ) admissions_first\n",
        "                    ON inputevents_cv.subject_id = admissions_first.subject_id\n",
        "                    AND inputevents_cv.hadm_id = admissions_first.hadm_id\n",
        "                    AND inputevents_cv.charttime >= (first_admittime)\n",
        "                    AND inputevents_cv.charttime <= DATE_ADD(first_admittime, INTERVAL 42 HOUR)\n",
        "                WHERE itemid IN ({', '.join(map(str, itemid))})\n",
        "                UNION ALL\n",
        "                SELECT inputevents_mv.subject_id, inputevents_mv.hadm_id, inputevents_mv.starttime as charttime FROM `physionet-data.mimiciii_clinical.inputevents_mv` inputevents_mv\n",
        "                JOIN (\n",
        "                    SELECT subject_id, hadm_id, MIN(admittime) AS first_admittime\n",
        "                    FROM `physionet-data.mimiciii_clinical.admissions`\n",
        "                    GROUP BY subject_id, hadm_id\n",
        "                ) admissions_first\n",
        "                    ON inputevents_mv.subject_id = admissions_first.subject_id\n",
        "                    AND inputevents_mv.hadm_id = admissions_first.hadm_id\n",
        "                    AND inputevents_mv.starttime >= (first_admittime)\n",
        "                    AND inputevents_mv.starttime <= DATE_ADD(first_admittime, INTERVAL 42 HOUR)\n",
        "                WHERE itemid IN ({', '.join(map(str, itemid))})\n",
        "            )\n",
        "        \"\"\"\n",
        "        query_job = client.query(query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "        # filter only data relevant to each patients first admission & first 42 hours\n",
        "        filtered_df = pd.merge(query_job, hosps, on=['subject_id', 'hadm_id'], how='right')\n",
        "        filtered_df = filtered_df[\n",
        "            (filtered_df['charttime'] >= filtered_df['admittime']) &\n",
        "            (filtered_df['charttime'] <= filtered_df['dischtime'])\n",
        "        ]\n",
        "        subject_ids = filtered_df['subject_id'].unique()\n",
        "        mask = medications_df['subject_id'].isin(subject_ids)\n",
        "        medications_df.loc[mask, category.lower()] = 1\n",
        "\n",
        "medications_df = medications_df.sort_values(by='subject_id')\n",
        "medications_df.to_csv('medications_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBz0ffiUJqBY"
      },
      "source": [
        "Retrieve procedure events information for patients during the first 42 hours of their admission. Again, we look at different procedures performed on patients in the first 42 hours of their admission and mark it as a feature for later, using a boolean matrix with subject_id for rows and the different procedures as columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "paVTFjp1mPdN"
      },
      "outputs": [],
      "source": [
        "# @title Query DB - Procedure Events\n",
        "\n",
        "procedureevents_mv_query = \\\n",
        "\"\"\"\n",
        "SELECT DISTINCT procedureevents_mv.subject_id, procedureevents_mv.hadm_id, procedureevents_mv.starttime, ordercategoryname\n",
        "FROM `physionet-data.mimiciii_clinical.procedureevents_mv` procedureevents_mv\n",
        "JOIN `physionet-data.mimiciii_clinical.admissions` admissions\n",
        "    ON procedureevents_mv.subject_id = admissions.subject_id\n",
        "    AND procedureevents_mv.starttime >= (admissions.admittime)\n",
        "    AND procedureevents_mv.starttime <= DATE_ADD(admissions.admittime, INTERVAL 42 HOUR)\n",
        "ORDER BY procedureevents_mv.subject_id;\n",
        "\"\"\"\n",
        "query_job = client.query(procedureevents_mv_query).result().to_dataframe().rename(str.lower, axis='columns')\n",
        "\n",
        "# filter only data relevant to each patients first admission & first 42 hours\n",
        "filtered_df = pd.merge(query_job, hosps, on=['subject_id', 'hadm_id'], how='right')\n",
        "filtered_df = filtered_df[\n",
        "    (filtered_df['starttime'] >= filtered_df['admittime']) &\n",
        "    (filtered_df['starttime'] <= filtered_df['dischtime'])\n",
        "]\n",
        "filtered_df = filtered_df[['subject_id', 'ordercategoryname']]\n",
        "\n",
        "columns = list(filtered_df['ordercategoryname'].unique())\n",
        "columns = list(set(value.lower() for value in columns))\n",
        "procedure_events_df = pd.DataFrame(columns=['subject_id'] + columns)\n",
        "procedure_events_df['subject_id'] = hosps['subject_id'].unique()\n",
        "procedure_events_df.fillna(0, inplace=True)\n",
        "\n",
        "for category in columns:\n",
        "    subject_ids = filtered_df.loc[filtered_df['ordercategoryname'].str.lower() == category, 'subject_id'].tolist()\n",
        "    mask = procedure_events_df['subject_id'].isin(subject_ids)\n",
        "    procedure_events_df.loc[mask, category.lower()] = 1\n",
        "\n",
        "procedure_events_df = procedure_events_df.sort_values(by='subject_id')\n",
        "procedure_events_df.to_csv('procedure_events_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaJKH6BcKWum"
      },
      "source": [
        "Merging all of our data together: admission data with target labels, procedure events, medications, vitals and lab tests, sapsii and sofa features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CXDj_ZQzV3Kt"
      },
      "outputs": [],
      "source": [
        "# merged_df = hosps\n",
        "merged_df = hosps.merge(procedure_events_df, how='left', on=['subject_id'])\n",
        "merged_df = merged_df.merge(medications_df, how='left', on=['subject_id'])\n",
        "\n",
        "vits['category'] = 'vits'\n",
        "vits_and_labs_df = pd.concat([vits, labs])\n",
        "\n",
        "vits_and_labs_df['feature name'] = vits_and_labs_df['feature name'].str.lower()\n",
        "grouped = vits_and_labs_df.groupby(['hadm_id', 'feature name', pd.Grouper(key='charttime', freq='42H')])                    # group by 'hadm_id', 'feature_name', and daily intervals\n",
        "aggregated = grouped['valuenum'].agg(['mean', 'max', 'min'])                                                                # mean, max, and min for each group\n",
        "pivoted = aggregated.pivot_table(index=['hadm_id'], columns='feature name', values=['mean', 'max', 'min'])                  # reshape\n",
        "pivoted.columns = ['_'.join(col).rstrip('_') for col in pivoted.columns]                                                    # change names to min_feature, max_feature...\n",
        "pivoted.reset_index(inplace=True)\n",
        "merged_df = merged_df.merge(pivoted, how='left', on=['hadm_id'])\n",
        "\n",
        "# fix weight column\n",
        "weights = vits.loc[vits['feature name'] == 'Weight'].groupby(['subject_id'])['valuenum'].agg(['first'])\n",
        "weights.rename(columns={'first': 'weight'}, inplace=True)\n",
        "weights.reset_index(inplace=True)\n",
        "weights.rename(columns={'index': 'subject_id'}, inplace=True)\n",
        "merged_df = merged_df.drop(['min_weight', 'max_weight', 'mean_weight'], axis=1)\n",
        "merged_df = merged_df.merge(weights, how='left', on='subject_id')\n",
        "\n",
        "# add sofa and sapsii\n",
        "merged_df = merged_df.merge(sofa, how='left', on=['subject_id', 'hadm_id'])\n",
        "merged_df = merged_df.merge(sapsii, how='left', on=['subject_id', 'hadm_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuhJamAZd8LQ"
      },
      "source": [
        "Dropping all columns that might cause data leakage, and splitting the data to train and test according to given subject ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BMuiLc-BQTmJ"
      },
      "outputs": [],
      "source": [
        "# @title Preprocess\n",
        "\n",
        "target_labels = ['mortality_label', 'prolonged_stay_label', 'readmit_label']\n",
        "columns_to_drop = ['admittime', 'dischtime', 'ethnicity', 'dob', 'dod', 'deathtime', 'mort', 'hadm_id', 'los_hosp_hr']\n",
        "numerical_columns = ['age', 'weight', 'max_albumin', 'max_anion gap', 'max_bicarbonate', 'max_bilirubin', 'max_bun', 'max_chloride', 'max_creatinine',\n",
        "     'max_diasbp', 'max_glucose', 'max_heartrate', 'max_hematocrit', 'max_hemoglobin', 'max_inr', 'max_lactate',\n",
        "     'max_magnesium', 'max_meanbp', 'max_phosphate', 'max_platelet', 'max_potassium', 'max_pt', 'max_ptt',\n",
        "     'max_resprate', 'max_sodium', 'max_spo2', 'max_sysbp', 'max_tempc', 'max_wbc', 'mean_albumin', 'mean_anion gap',\n",
        "     'mean_bicarbonate', 'mean_bilirubin', 'mean_bun', 'mean_chloride', 'mean_creatinine', 'mean_diasbp',\n",
        "     'mean_glucose', 'mean_heartrate', 'mean_hematocrit', 'mean_hemoglobin', 'mean_inr', 'mean_lactate',\n",
        "     'mean_magnesium', 'mean_meanbp', 'mean_phosphate', 'mean_platelet', 'mean_potassium', 'mean_pt', 'mean_ptt',\n",
        "     'mean_resprate', 'mean_sodium', 'mean_spo2', 'mean_sysbp', 'mean_tempc', 'mean_wbc', 'min_albumin',\n",
        "     'min_anion gap', 'min_bicarbonate', 'min_bilirubin', 'min_bun', 'min_chloride', 'min_creatinine', 'min_diasbp',\n",
        "     'min_glucose', 'min_heartrate', 'min_hematocrit', 'min_hemoglobin', 'min_inr', 'min_lactate', 'min_magnesium',\n",
        "     'min_meanbp', 'min_phosphate', 'min_platelet', 'min_potassium', 'min_pt', 'min_ptt', 'min_resprate', 'min_sodium',\n",
        "     'min_spo2', 'min_sysbp', 'min_tempc', 'min_wbc']\n",
        "\n",
        "filtered_df = merged_df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "# filter initial cohort\n",
        "init_cohort = pd.read_csv('initial_cohort.csv')\n",
        "df_train = pd.merge(filtered_df, init_cohort, on='subject_id', how='inner')\n",
        "X_train = df_train.drop(['subject_id'] + target_labels, axis=1)\n",
        "y_train = df_train[target_labels]\n",
        "\n",
        "# filter test cohort\n",
        "test_cohort = pd.read_csv('test_example.csv')\n",
        "df_test = pd.merge(filtered_df, test_cohort, on='subject_id', how='inner')\n",
        "X_test = df_test.drop(['subject_id'] + target_labels, axis=1)\n",
        "y_test = df_test[target_labels]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBKOClhVeU6L"
      },
      "source": [
        "Impute data according to patients' age-gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TtIlK3ZnKaDa"
      },
      "outputs": [],
      "source": [
        "class AgeGenderImputer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, columns):\n",
        "        self.imputers = None\n",
        "        self.columns = columns\n",
        "\n",
        "    # init imputers based on X (train)\n",
        "    def init_imputers(self, X):\n",
        "        X = X.copy()\n",
        "        self.imputers = {}\n",
        "        X['age_group'] = pd.cut(X['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69', '70_79', '80p'], right = False)\n",
        "        for (age, gender), group in X.groupby(['age_group', 'gender'])[self.columns]:\n",
        "            imputer = SimpleImputer(strategy='mean', keep_empty_features=True)\n",
        "            imputer.fit(group)\n",
        "            self.imputers[(age, gender)] = imputer\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.imputers is None:\n",
        "            self.init_imputers(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_copy = X.copy()\n",
        "        X_copy['age_group'] = pd.cut(X_copy['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69', '70_79', '80p'], right = False)\n",
        "        for (age, gender), group in X_copy.groupby(['age_group', 'gender'])[self.columns]:\n",
        "            if (age, gender) in self.imputers:\n",
        "                unique_rows = group[~group.index.duplicated(keep='first')]\n",
        "                imputed_group = self.imputers[(age, gender)].transform(group)\n",
        "                X_copy.loc[unique_rows.index, self.columns] = imputed_group\n",
        "        X_copy = X_copy.drop('age_group', axis=1)\n",
        "        return X_copy\n",
        "\n",
        "# impute\n",
        "def perform_imputation(x_t, x_v):\n",
        "    age_gender_imputer = AgeGenderImputer(x_t.columns.to_list())\n",
        "\n",
        "    # fit the imputer on training data\n",
        "    age_gender_imputer.fit(x_t)\n",
        "\n",
        "    # impute both DFs\n",
        "    x_t_imputed = age_gender_imputer.transform(x_t)\n",
        "    x_v_imputed = age_gender_imputer.transform(x_v)\n",
        "\n",
        "    return x_t_imputed, x_v_imputed\n",
        "\n",
        "# scaler - no need with XGBoost\n",
        "def perform_scale(x_t, x_v):\n",
        "    scaler = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('numerical', StandardScaler(), x_t.columns.to_list())\n",
        "            ], remainder='passthrough')\n",
        "    x_t_scaled = scaler.fit_transform(x_t)\n",
        "    x_v_scaled = scaler.fit_transform(x_v)\n",
        "    return x_t_scaled, x_v_scaled\n",
        "\n",
        "# impute and scale\n",
        "def impute_and_scale(x_t, x_v, partial_columns = None):\n",
        "    x_t_imputed, x_v_imputed = perform_imputation(x_t, x_v)\n",
        "    return x_t_imputed, x_v_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X00aTmzoVHuM"
      },
      "outputs": [],
      "source": [
        "# @title Imputation\n",
        "\n",
        "def impute(group):\n",
        "    \"\"\"\n",
        "    takes in a pandas group, and replaces the\n",
        "    null value with the mean of the none null\n",
        "    values of the same group\n",
        "    \"\"\"\n",
        "    mask = group.isnull()\n",
        "    group[mask] = group[~mask].mean()\n",
        "    return group\n",
        "\n",
        "filtered_columns = [column for column in numerical_columns if column != 'age']\n",
        "X_train['age_group'] = pd.cut(X_train['age'], [0, 18, 30, 40, 50, 60, 70, 80, 400],\n",
        "   labels = ['0_17', '18_29', '30_39', '40_41', '50_59', '60_69',  '70_79', '80p'], right = False)\n",
        "\n",
        "for item in filtered_columns:\n",
        "    try:\n",
        "        X_train[item.lower()] = X_train.groupby(['age_group', 'gender'])[item.lower()].transform(impute)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "X_train = X_train.drop('age_group', axis=1)\n",
        "\n",
        "# imputation for sofa, according to the creators of the SOFA score\n",
        "X_train['sofa'] = X_train['sofa'].fillna(0)\n",
        "X_train['sapsii'] = X_train['sapsii'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VMuEF2Oberb8"
      },
      "outputs": [],
      "source": [
        "# best params for the XGBoost model of each task\n",
        "\n",
        "# best_params = {'readmit_label': {'max_depth': 3,\n",
        "#                                  'min_child_weight': 3,\n",
        "#                                  'learning_rate': 0.1,\n",
        "#                                  'n_estimators': 100,\n",
        "#                                  'colsample_bytree': 1,\n",
        "#                                  'gamma': 0.1,\n",
        "#                                  'subsample': 1},\n",
        "#                'prolonged_stay_label': {'max_depth': 6,\n",
        "#                                         'min_child_weight': 1,\n",
        "#                                         'learning_rate': 0.1,\n",
        "#                                         'n_estimators': 100,\n",
        "#                                         'colsample_bytree': 0.75,\n",
        "#                                         'gamma': 0.1,\n",
        "#                                         'subsample': 0.75},\n",
        "#                'mortality_label': {'max_depth': 4,\n",
        "#                                     'min_child_weight': 5,\n",
        "#                                     'learning_rate': 0.1,\n",
        "#                                     'n_estimators': 150,\n",
        "#                                     'colsample_bytree': 0.75,\n",
        "#                                     'gamma': 0.1,\n",
        "#                                     'subsample': 0.75}}\n",
        "\n",
        "best_params = {'readmit_label': {'max_depth': 3,\n",
        "                                 'min_child_weight': 1,\n",
        "                                 'learning_rate': 0.1,\n",
        "                                 'n_estimators': 50,\n",
        "                                 'colsample_bytree': 0.75,\n",
        "                                 'gamma': 0.1,\n",
        "                                 'subsample': 0.75,\n",
        "                                 'reg_alpha': 5},\n",
        "               'prolonged_stay_label': {'max_depth': 6,\n",
        "                                        'min_child_weight': 1,\n",
        "                                        'learning_rate': 0.1,\n",
        "                                        'n_estimators': 100,\n",
        "                                        'colsample_bytree': 1,\n",
        "                                        'gamma': 0.1,\n",
        "                                        'subsample': 1,\n",
        "                                        'reg_alpha': 10},\n",
        "               'mortality_label': {'max_depth': 7,\n",
        "                                    'min_child_weight': 5,\n",
        "                                    'learning_rate': 0.1,\n",
        "                                    'n_estimators': 200,\n",
        "                                    'colsample_bytree': 1,\n",
        "                                    'gamma': 0.2,\n",
        "                                    'subsample': 0.75,\n",
        "                                    'reg_alpha': 10}}\n",
        "\n",
        "# {'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 50, 'reg_alpha': 5}\n",
        "# {'colsample_bytree': 0.75, 'gamma': 0.1, 'subsample': 0.75}\n",
        "# {'learning_rate': 0.1, 'max_depth': 6, 'min_child_weight': 1, 'n_estimators': 100, 'reg_alpha': 10}\n",
        "# {'colsample_bytree': 1, 'gamma': 0.1, 'subsample': 1}\n",
        "# {'learning_rate': 0.1, 'max_depth': 7, 'min_child_weight': 5, 'n_estimators': 200, 'reg_alpha': 10}\n",
        "# {'colsample_bytree': 1, 'gamma': 0.2, 'subsample': 0.75}\n",
        "\n",
        "def roc_plot(model, x, y, label):\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    fig = plt.figure(figsize=(7,7))\n",
        "    roc_aucs = []\n",
        "\n",
        "    pred_prob = model.predict_proba(x)\n",
        "    fpr, tpr, thresholds = roc_curve(y, pred_prob[:, 1])\n",
        "    mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "    mean_tpr[0] = 0.0\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    roc_aucs.append(roc_auc)\n",
        "    plt.plot(fpr, tpr, lw=2, label='ROC (area = %0.2f)' % (roc_auc))\n",
        "\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    plt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Luck')\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.title('ROC curve: ' + label)\n",
        "\n",
        "    fig.savefig(plots_directory + '/ROC_' + label + '.png')\n",
        "\n",
        "def precision_recall_plot(model, x, y, label):\n",
        "    y_prob = model.predict_proba(x)[:, 1]\n",
        "    precision, recall, _ = precision_recall_curve(y, y_prob)\n",
        "    avg_precision = average_precision_score(y, y_prob)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve - ' + label)\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f'Average Precision Score: {avg_precision:.2f}')\n",
        "\n",
        "def feature_importance(model, X_train):\n",
        "    feature_importances = model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "    feature_importance_df = feature_importance_df.head(10)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title('Feature Importances - ' + label)\n",
        "    plt.show()\n",
        "\n",
        "def k_fold_roc(params, label, X_t, Y_t, K=5, fig=None, ax=None):\n",
        "\n",
        "    if fig is None or ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    mean_tpr = 0.0\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "\n",
        "    eval_size = int(np.round(1. / K))\n",
        "    skf = StratifiedKFold(n_splits=K)\n",
        "    roc_aucs = []\n",
        "\n",
        "    for index, (train_indices, test_indices) in enumerate(skf.split(X_t, Y_t)):\n",
        "        x_t, y_t = X_t.iloc[train_indices], Y_t.iloc[train_indices]\n",
        "        x_v, y_v = X_t.iloc[test_indices], Y_t.iloc[test_indices]\n",
        "\n",
        "        scale_weight = 1. * y_t.value_counts()[0] / y_t.value_counts()[1]\n",
        "        params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "        model = XGBClassifier()\n",
        "        model.set_params(**params)\n",
        "\n",
        "        x_t, x_v = perform_imputation(x_t, x_v)\n",
        "\n",
        "        # model.fit(x_t, y_t)\n",
        "        # calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
        "        # calibrated_model.fit(x_v, y_v)\n",
        "\n",
        "        calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=3)\n",
        "        calibrated_model.fit(x_t, y_t)\n",
        "\n",
        "        pred_prob = calibrated_model.predict_proba(x_v)\n",
        "        fpr, tpr, thresholds = roc_curve(y_v, pred_prob[:, 1])\n",
        "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "        mean_tpr[0] = 0.0\n",
        "\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_aucs.append(roc_auc)\n",
        "        ax.plot(fpr, tpr, lw=2, label='ROC fold %d (area = %0.2f)' % (index + 1, roc_auc))\n",
        "\n",
        "    mean_tpr /= K\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    ax.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
        "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='Luck')\n",
        "\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.set_ylim([0, 1])\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_title('ROC curve: ' + label)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def my_plot_importance(booster, x_t, figsize, ax=None, **kwargs):\n",
        "    columns = x_t.columns.to_list()\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "\n",
        "    booster.get_booster().feature_names = columns\n",
        "    plot_importance(booster=booster, ax=ax, max_num_features=20, importance_type='weight', **kwargs)\n",
        "\n",
        "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "        item.set_fontsize(10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def calibration_curve_plt(calibrated_pipeline, x_v, y_v, ax=None):\n",
        "    y_pred_proba = calibrated_pipeline.predict_proba(x_v)\n",
        "    prob_true, prob_pred = calibration_curve(y_v, y_pred_proba[:, 1], n_bins=10)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "    ax.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\n",
        "    ax.plot([0, 1], [0, 1], ls='--', label='Perfectly Calibrated')\n",
        "    ax.set_xlabel('Mean Predicted Probability')\n",
        "    ax.set_ylabel('Fraction of Positives')\n",
        "    ax.set_title('Calibration Curve')\n",
        "    ax.legend()\n",
        "    return fig, ax\n",
        "\n",
        "def plot_shap(model, x_t, columns, label, fig=None, ax=None):\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    explainer = shap.Explainer(model)\n",
        "    shap_values = explainer.shap_values(x_t)\n",
        "    shap.summary_plot(shap_values, x_t, feature_names=columns, show=False, title='SHAP plot for ' + label, plot_size=(24,8))\n",
        "\n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZetcfKPlxHT"
      },
      "outputs": [],
      "source": [
        "for label in y_train.columns.tolist()[::-1]:\n",
        "    x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "    x_t, x_v = perform_imputation(x_t, x_v)\n",
        "\n",
        "    params = best_params[label]\n",
        "    scale_weight = 1.*y_t.value_counts()[0]/y_t.value_counts()[1]\n",
        "    params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "    model = XGBClassifier()\n",
        "    model.set_params(**params)\n",
        "    model.fit(x_t, y_t)\n",
        "    calibrated_model = CalibratedClassifierCV(model, method='sigmoid', cv=4)\n",
        "    calibrated_model.fit(x_v, y_v)\n",
        "\n",
        "    # plots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(30, 8))\n",
        "\n",
        "    k_fold_roc(params, label, X_train, y_train[label], fig=fig, ax=axes[0])\n",
        "    calibration_curve_plt(calibrated_model, x_t, y_t, ax=axes[1])\n",
        "    plot_shap(model, X_train, X_train.columns.to_list(), label, fig=fig, ax=axes[2])\n",
        "\n",
        "    plt.savefig(plots_directory + '/plots_' + label + '.png')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IK_ER14twrDP"
      },
      "outputs": [],
      "source": [
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkhnWsqW2wIV"
      },
      "outputs": [],
      "source": [
        "# @title Test performance on leftover data - REMOVE THIS\n",
        "# performance on readmission on really low, we need to improve it\n",
        "\n",
        "df_custom = filtered_df[~filtered_df['subject_id'].isin(init_cohort['subject_id'])]\n",
        "X_custom = df_custom.drop(['subject_id'] + target_labels, axis=1)\n",
        "y_custom = df_custom[target_labels]\n",
        "\n",
        "for label in y_train.columns.tolist()[::-1]:\n",
        "\n",
        "    x_t, x_c = perform_imputation(X_train.astype('Float64', copy=False), X_custom.astype('Float64', copy=False))\n",
        "    y_t, y_c = y_train[label], y_custom[label]\n",
        "\n",
        "    x_t, x_v, y_t, y_v = train_test_split(x_t, y_t, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "    params = best_params[label]\n",
        "    scale_weight = 1.*y_t.value_counts()[0]/y_t.value_counts()[1]\n",
        "    # params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "    model = XGBClassifier()\n",
        "    model.set_params(**params)\n",
        "    model.fit(x_t, y_t)\n",
        "    calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
        "    calibrated_model.fit(x_v, y_v)\n",
        "\n",
        "    precision_recall_plot(calibrated_model, x_c, y_c, label)\n",
        "    roc_plot(calibrated_model, x_c, y_c, label)\n",
        "\n",
        "    # print(classification_report(y_true = y_t, y_pred = model.predict(x_t)))\n",
        "    # print(classification_report(y_true = y_c, y_pred = model.predict(x_c)))\n",
        "    # print(classification_report(y_true = y_v, y_pred = model.predict(x_v)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ukBzqrxm5p"
      },
      "outputs": [],
      "source": [
        "# @title Find best XGBoost model params\n",
        "from google.colab import files\n",
        "\n",
        "labels = ['readmit_label', 'prolonged_stay_label', 'mortality_label']\n",
        "for label in labels:\n",
        "    best_params = {}\n",
        "    # x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "    x_t, y_t = X_train, y_train[label]\n",
        "\n",
        "    scale_weight = 1.*y_t.value_counts()[0]/y_t.value_counts()[1]\n",
        "    best_params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "    model = XGBClassifier()\n",
        "    model.set_params(**best_params)\n",
        "\n",
        "    params = {\n",
        "        'max_depth': [3, 4, 5, 6, 7],\n",
        "        'min_child_weight': [1, 3, 5],\n",
        "        'n_estimators': [50, 100, 150, 200],\n",
        "        'learning_rate': [0.1],\n",
        "        'reg_alpha': [0, 2, 5, 10]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=model, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    best_params.update(cv_model.best_params_)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    params = {\n",
        "        'gamma': [0.1, 0.2],\n",
        "        'subsample': [0.5, 0.75, 1],\n",
        "        'colsample_bytree': [0.75, 1]\n",
        "    }\n",
        "    cv_model = GridSearchCV(estimator=cv_model.best_estimator_, param_grid=params, cv=3, scoring='roc_auc')\n",
        "    cv_model.fit(x_t, y_t)\n",
        "    best_params.update(cv_model.best_params_)\n",
        "    print(cv_model.best_params_)\n",
        "\n",
        "    output_file = 'best_params_' + label + '.json'\n",
        "\n",
        "    # Write the merged_dict to a JSON file\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(best_params, json_file)\n",
        "    files.download(output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(output_file)"
      ],
      "metadata": {
        "id": "6YZlgr1-TaO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvwgVb9FcMQ8"
      },
      "outputs": [],
      "source": [
        "# @title With least important features removal\n",
        "# for label in y_train.columns.tolist()[::-1]:\n",
        "#     X_train_selected = X_train\n",
        "#     x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "#     x_t, x_v = perform_imputation(x_t, x_v)\n",
        "\n",
        "#     params = best_params[label]\n",
        "#     scale_weight = 1.*y_t.value_counts()[0]/y_t.value_counts()[1]\n",
        "#     params['scale_pos_weight'] = scale_weight\n",
        "\n",
        "#     model = XGBClassifier()\n",
        "#     model.set_params(**params)\n",
        "#     model.fit(x_t, y_t)\n",
        "\n",
        "#     # least important features removal\n",
        "#     feature_importances = model.feature_importances_\n",
        "#     num_features_to_remove = int(0.4 * len(feature_importances))  # % of features\n",
        "#     least_important_indices = np.argsort(feature_importances)[:num_features_to_remove]\n",
        "#     least_important_indices = least_important_indices[least_important_indices != X_train.columns.get_loc('gender')]\n",
        "#     least_important_indices = least_important_indices[least_important_indices != X_train.columns.get_loc('age')]\n",
        "#     X_train_selected = X_train.drop(X_train.columns[least_important_indices], axis=1)\n",
        "#     # train again\n",
        "#     x_t, x_v, y_t, y_v = train_test_split(X_train_selected, y_train[label], test_size=0.1, random_state=42)\n",
        "#     model = XGBClassifier()\n",
        "#     model.set_params(**params)\n",
        "#     model.fit(x_t, y_t)\n",
        "\n",
        "#     calibrated_model = CalibratedClassifierCV(model, method='isotonic', cv='prefit')\n",
        "#     calibrated_model.fit(x_v, y_v)\n",
        "\n",
        "#     # plots\n",
        "#     my_plot_importance(model, x_t, (4,8), X_train_selected.columns.to_list())\n",
        "#     calibration_curve_plt(calibrated_model, x_t, y_t)\n",
        "#     feature_importances = plot_shap(model, x_t, X_train_selected.columns.to_list(), label)\n",
        "\n",
        "#     # # remove least important #2\n",
        "#     # num_features_to_remove = int(0.4 * len(feature_importances))\n",
        "#     # least_important_indices = np.argsort(feature_importances)[:num_features_to_remove]\n",
        "#     # least_important_indices = least_important_indices[least_important_indices != X_train.columns.get_loc('gender')]\n",
        "#     # least_important_indices = least_important_indices[least_important_indices != X_train.columns.get_loc('age')]\n",
        "#     # X_train_selected = X_train.drop(X_train.columns[least_important_indices], axis=1)\n",
        "\n",
        "\n",
        "#     k_fold_roc(params, label, X_train_selected, y_train[label], 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHZrK3GurMhh"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# for label in y_train.columns.tolist()[::-1]:\n",
        "#     # Split the data into training and testing sets\n",
        "#     x_t, x_v, y_t, y_v = train_test_split(X_train, y_train[label], test_size=0.2, random_state=42)\n",
        "\n",
        "#     # Define the parameter grid for random search\n",
        "#     scale_weight = 1.*y_t.value_counts()[0]/y_t.value_counts()[1]\n",
        "#     param_grid = {\n",
        "#         'scale_pos_weight': [scale_weight],\n",
        "#         'learning_rate': [0.01, 0.5, 0.1, 0.2],\n",
        "#         'n_estimators': [100, 200, 300],\n",
        "#         'max_depth': [3, 4, 5, 6, 7],\n",
        "#         'min_child_weight': [1, 2, 3, 4],\n",
        "#         'subsample': [0.8, 0.9, 1.0],\n",
        "#         'gamma': [0.1, 0.3, 0.5],\n",
        "#         'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "#     }\n",
        "\n",
        "#     # Create an XGBClassifier\n",
        "#     xgb_model = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "#     # Create RandomizedSearchCV instance\n",
        "#     random_search = RandomizedSearchCV(\n",
        "#         xgb_model,\n",
        "#         param_distributions=param_grid,\n",
        "#         n_iter=10,  # Number of parameter settings to sample\n",
        "#         scoring='roc_auc',  # Use appropriate metric for your problem\n",
        "#         cv=3,  # Number of cross-validation folds\n",
        "#         verbose=4,  # Higher values give more output\n",
        "#         n_jobs=-1,  # Use all available CPU cores\n",
        "#         random_state=42\n",
        "#     )\n",
        "\n",
        "#     # Perform the random search on the training data\n",
        "#     random_search.fit(x_t, y_t)\n",
        "\n",
        "#     # Print the best parameters and best score\n",
        "#     print(\"Best Parameters:\", random_search.best_params_)\n",
        "#     print(\"Best Score:\", random_search.best_score_)\n",
        "\n",
        "#     # Evaluate the best model on the test set\n",
        "#     best_model = random_search.best_estimator_\n",
        "#     test_accuracy = best_model.score(X_test, y_test)\n",
        "#     print(\"Test Accuracy of Best Model:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALIa5-KAw-VU"
      },
      "outputs": [],
      "source": [
        "# # some stats\n",
        "\n",
        "# df = pd.concat([X_train, y_train], axis = 1)\n",
        "\n",
        "# # mortality in different ethnicities\n",
        "# for eth in ['eth_white', 'eth_black', 'eth_hispanic', 'eth_asian', 'eth_other']:\n",
        "#     text = eth + ': '\n",
        "#     count = df[(df[eth] == 1) & (df['mortality_label'] == 1)].shape[0]\n",
        "#     text += str(count) + ' died, out of ' + str(df[(df[eth] == 1)].shape[0]) + '. That\\'s '\n",
        "#     percentage = round((count / df[(df[eth] == 1)].shape[0]) * 100, 2)\n",
        "#     total_percentage = round(count / df[(df['mortality_label'] == 1)].shape[0] * 100, 2)\n",
        "#     text += str(percentage) + '%, and ' + str(total_percentage) + '% of all dead'\n",
        "#     print(text)\n",
        "\n",
        "# # prolonged stay with newborns\n",
        "# count = df[(df['admission_type_NEWBORN'] == 1) & (df['prolonged_stay_label'] == 1)].shape[0]\n",
        "# percentage = round(count / df[(df['prolonged_stay_label'] == 1)].shape[0] * 100, 2)\n",
        "# print(percentage, count, df[(df['prolonged_stay_label'] == 1)].shape[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCYaBsoKrlzl7ZTnr6hSvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}